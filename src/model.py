#!/usr/bin/env python3
import os
import random
import time
from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

class MyModel:
    """
    Byte-level next-character predictor using EvaByte.
    """

    def __init__(self):
        # Load the EvaByte tokenizer & model (byte-level, causal)
        self.tokenizer = AutoTokenizer.from_pretrained(
            "evabyte/EvaByte", trust_remote_code=True
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            "evabyte/EvaByte",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True,
            device_map="auto"
        ).eval()

        # Configure tokenizer to truncate left (keep last bytes) and pad right
        self.tokenizer.truncation_side = "left"
        self.tokenizer.padding_side    = "right"
    
    @classmethod
    def load_training_data(cls):
        # your code here
        # this particular model doesn't train
        return []

    @classmethod
    def load_test_data(cls, fname):
        # your code here
        data = []
        with open(fname) as f:
            for line in f:
                inp = line[:-1]  # the last character is a newline
                data.append(inp)
        return data

    @classmethod
    def write_pred(cls, preds, fname):
        with open(fname, 'wt') as f:
            for p in preds:
                f.write('{}\n'.format(p))

    def run_train(self, data, work_dir):
        # your code here
        pass

    def run_pred(self, data, batch_size: int = 4, max_length: int = 2):
        """
        Minibatched byte-level inference with EvaByte.
        Predicts exactly one byte per prompt, which you can decode
        directly into the next character (for ASCII).
        """
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(device)

        if device.type == "cuda":
            torch.cuda.reset_peak_memory_stats(device)

        preds = []
        t0 = time.time()
        total = len(data)

        for start_idx in range(0, total, batch_size):
            batch = data[start_idx : start_idx + batch_size]
            print(f"Batch {start_idx//batch_size+1}/{(total-1)//batch_size+1} "
                  f"({start_idx}/{total} prompts)")

            # 1) Tokenize batch of raw strings -> byte tokens
            enc = self.tokenizer(
                batch,
                return_tensors="pt",
                padding="longest",
                truncation=True,
                max_length=max_length,
                return_attention_mask=True
            ).to(device, non_blocking=True)

            # 2) Generate exactly 1 new token (byte) per prompt
            with torch.inference_mode():
                out_ids = self.model.generate(
                    input_ids=enc.input_ids,
                    attention_mask=enc.attention_mask,
                    max_new_tokens=1,
                    do_sample=False,
                    use_cache=False,            # disable kv-cache to avoid Triton assertions
                )
                # out_ids.shape == (batch_size, seq_len+1)

            # 3) Extract the newly generated byte for each example
            seq_len = enc.input_ids.size(1)
            gen_ids = out_ids[:, seq_len]  # the byte at position seq_len

            # 4) Decode each byte token into a (single) character string
            for byte_id in gen_ids:
                ch = self.tokenizer.decode(
                    [int(byte_id)],
                    clean_up_tokenization_spaces=False,
                    skip_special_tokens=False
                )
                preds.append(ch)

        # Final stats
        elapsed = time.time() - t0
        print(f"Done {total} samples in {elapsed:.2f}s "
              f"({total/elapsed:.2f} samples/sec)")
        if device.type == "cuda":
            peak_gb = torch.cuda.max_memory_allocated(device) / (1024 ** 3)
            print(f"Peak GPU memory allocated: {peak_gb:.2f} GB")

        return preds

    def run_pred_batch(self, data, batch_size: int = 32):
        """
        Minibatched inference:

        - byt5-xl: 
        """
        # TODO: cpu mode isn't quite ready -- need some changes to make apple mps (metal performance shaders) work.
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        if device == "cuda":
            torch.cuda.reset_peak_memory_stats(device)
        preds = []
        start = time.time()
        num_prompts = len(data)
        batch_count = 0
        for idx in range(0, num_prompts, batch_size):
            batch_count += 1
            start_batch = time.time()
            batch_prompts = data[idx : idx + batch_size]
            done = min(idx, num_prompts)
            print(f"batch {idx / batch_size}/{len(data) // batch_size}; {done}/{num_prompts} inferences")

            # Tokenize the whole batch, pad to longest in batch
            encoded = self.tokenizer(
                batch_prompts,
                return_tensors="pt",
                truncation=True,
                max_length=1024,
                padding=True,
            ).to(device)
            

            """
            with torch.inference_mode():
                output_ids = self.model.generate(
                    **encoded,
                    max_new_tokens=1,
                    do_sample=False
                )
            """
            with torch.inference_mode():
                out_ids = self.model.generate(
                    input_ids=encoded.input_ids,
                    attention_mask=encoded.attention_mask,
                    max_new_tokens=1,
                    do_sample=False,
                )

            for tid in out_ids[:, 0]:
                # skip_special_tokens=True will drop <pad>/<eos> if they appear
                preds.append(self.tokenizer.decode(
                    [int(tid)],
                    skip_special_tokens=True,
                    clean_up_tokenization_spaces=False
                ))
            # grab the generated token for each in the batch
            # output_ids shape: (batch, seq_len+1)
            # so we take output_ids[:, -1]
            # next_ids = output_ids[:, -1]
            # decode each single-token tensor
            """
            for tid in next_ids:
                preds.append(
                    self.tokenizer.decode(
                        [int(tid)],
                        clean_up_tokenization_spaces=False
                    )
                )
            """
            end_batch = time.time()
            elapsed_batch = end_batch - start_batch
            samples_per_sec_batch = batch_size / elapsed_batch
            print(f"minibatch with {batch_size} samples: {elapsed_batch * 1000:.03f} ms ({samples_per_sec_batch:.03f} samples/sec)")

        end = time.time()
        elapsed = end - start
        samples_per_sec = len(data) / elapsed
        if device == "cuda":
            torch.cuda.reset_peak_memory_stats(device)
            peak_gb = torch.cuda.max_memory_allocated(device) / (1024 ** 3)

        print(f"total inference time: {elapsed:.03f}s for {num_prompts} samples "
            f"({samples_per_sec:.03f} samples/sec)")
        if device == "cuda":
            print(f"peak GPU memory allocated: {peak_gb:.2f} GB")
        return preds

    def run_pred_single(self, data):
        """
        Predicts without batching:
        - 

        - with byt5-small: ~39 samples/sec without batching, peak 0.57GB mem allocated.
          - nvidia-smi on 20K examples: 849MiB/23034MiB, ~15-20%
        - with byt5-xl: ~16 samples/sec, 6.97GB peak. (~2x model), on 20K.
        """
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        torch.cuda.reset_peak_memory_stats(device)
        preds = []
        start = time.time()
        num_prompts = len(data)
        for idx, prompt in enumerate(data):
            if idx % 100 == 0:
                elapsed = time.time() - start
                samples_per_sec = idx / elapsed
                print(f"{idx}/{num_prompts} inferences done, {samples_per_sec:.03f} samples/sec")
            
            encoded = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=1024,  # arbitrary; default is 1e30.
            ).to(device)
            self.tokenizer.truncation_side = "left" # we're interested in rightmost bytes.

            with torch.inference_mode():
                output_ids = self.model.generate(
                    **encoded,
                    max_new_tokens=1,
                    do_sample=False
                )
            print(f"output_ids: {output_ids}")

            # pred_char = self.tokenizer.decode(next_token, clean_up_tokenization_spaces=False)
            # Get token IDs after <pad> (ID 0)
            generated_ids = output_ids[0][1:]  # skip the <pad> token

            if len(generated_ids) == 0:
                pred_char = ""
            else:
                pred_char = self.tokenizer.decode(
                    generated_ids,
                    skip_special_tokens=False,
                    clean_up_tokenization_spaces=False
                )
            preds.append(pred_char)

            print(f"prompt: {prompt}")
            print(f"pred_char: {pred_char}")

        print(f"prompt: {prompt}")
        print(f"preds: {preds}")
        end = time.time()
        elapsed = end - start
        samples_per_sec = len(data) / elapsed
        peak_gb = torch.cuda.max_memory_allocated(device) / (1024 ** 3)

        print(f"total inference time: {elapsed:.03f}s for {len(data)} samples "
            f"({samples_per_sec:.03f} samples/sec)")
        print(f"peak GPU memory allocated: {peak_gb:.2f} GB") # 
        return preds

    def save(self, work_dir):
        pass
        # your code here
        # this particular model has nothing to save, but for demonstration purposes we will save a blank file
        # with open(os.path.join(work_dir, 'model.checkpoint'), 'wt') as f:
            # f.write('dummy save')

    @classmethod
    def load(cls, work_dir):
        pass
        # your code here
        # this particular model has nothing to load, but for demonstration purposes we will load a blank file
        # with open(os.path.join(work_dir, 'model.checkpoint')) as f:
            # dummy_save = f.read()
        return MyModel()


if __name__ == '__main__':
    parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)
    parser.add_argument('mode', choices=('train', 'test'), help='what to run')
    parser.add_argument('--work_dir', help='where to save', default='work')
    parser.add_argument('--test_data', help='path to test data', default='example/input.txt')
    parser.add_argument('--test_output', help='path to write test predictions', default='pred.txt')
    args = parser.parse_args()

    random.seed(0)

    if args.mode == 'train':
        if not os.path.isdir(args.work_dir):
            print('Making working directory {}'.format(args.work_dir))
            os.makedirs(args.work_dir)
        print('Instatiating model')
        # model = MyModel()
        print('Loading training data')
        # train_data = MyModel.load_training_data()
        print('Training')
        # model.run_train(train_data, args.work_dir)
        print('Saving model')
        # model.save(args.work_dir)
    elif args.mode == 'test':
        print('Loading model')
        model = MyModel.load(args.work_dir)
        print('Loading test data from {}'.format(args.test_data))
        test_data = MyModel.load_test_data(args.test_data)
        print('Making predictions')
        pred = model.run_pred(test_data)
        print('Writing predictions to {}'.format(args.test_output))
        assert len(pred) == len(test_data), 'Expected {} predictions but got {}'.format(len(test_data), len(pred))
        model.write_pred(pred, args.test_output)
    else:
        raise NotImplementedError('Unknown mode {}'.format(args.mode))
